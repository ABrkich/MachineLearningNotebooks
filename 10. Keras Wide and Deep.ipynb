{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![UCI](http://mlr.cs.umass.edu/ml/assets/logo.gif)\n",
    "\n",
    "# Loading in the Data\n",
    "In this example, we are going to use crossed columns and embedding columns inside of a tensorflow object created with keras.\n",
    "\n",
    "However, we will start the process by loading up a dataset with a mix of categorical data and numeric data. This dataset is quite old and has been used many times in machine learning examples: the census data from 1990's. We will use it to predict if a person will earn over or under 50k per year.\n",
    "\n",
    "- https://archive.ics.uci.edu/ml/datasets/Census-Income+(KDD)\n",
    "- This data goes by various name, including the \"Adult\" dataset and 1996 census data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "headers = ['age','workclass','fnlwgt','education','edu_num','marital_status',\n",
    "           'occupation','relationship','race','sex','cap_gain','cap_loss','work_hrs_weekly','country','income']\n",
    "df_train_orig = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data',names=headers)\n",
    "df_test_orig = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test',names=headers)\n",
    "df_test_orig = df_test_orig.iloc[1:]\n",
    "print(df_train_orig.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "df_train = deepcopy(df_train_orig)\n",
    "df_test = deepcopy(df_test_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is organized as follows: \n",
    "\n",
    "|Variable | description|\n",
    "|----|--------|\n",
    "|age: | continuous|\n",
    "|workclass:      |Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, ...|\n",
    "|fnlwgt:         |continuous.|\n",
    "|education:      |Bachelors, Some-college, 11th, HS-grad, Prof-school, ...|\n",
    "|education-num:  |continuous.|\n",
    "|marital-status: |Married-civ-spouse, Divorced, Never-married, Separated, Widowed, ... |\n",
    "|occupation:     |Tech-support, Craft-repair, Other-service, ...|\n",
    "|relationship:   | Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.|\n",
    "|race:           |White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.|\n",
    "|sex:            |Female, Male.|\n",
    "|capital-gain:   |continuous.|\n",
    "|capital-loss:   |continuous.|\n",
    "|hours-per-week: |continuous.|\n",
    "|native-country: |United-States, Cambodia, England, ... |\n",
    "|**income (target):**|<50k, >=50k| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# let's just get rid of rows with any missing data\n",
    "# and then reset the indices of the dataframe so it corresponds to row number\n",
    "df_train.replace(to_replace=' ?',value=np.nan, inplace=True)\n",
    "df_train.dropna(inplace=True)\n",
    "df_train.reset_index()\n",
    "\n",
    "df_test.replace(to_replace=' ?',value=np.nan, inplace=True)\n",
    "df_test.dropna(inplace=True)\n",
    "df_test.reset_index()\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing\n",
    "For preprocessing, we are going to fix a few issues in the dataset. \n",
    "\n",
    "- This first includes the use of \"50K.\" instead of \"50K\" in the test set. \n",
    "- Next, we will encode the categorical features as integers (eventually we will deal with these integers like a one-hot encoding)\n",
    "- Finally, we will make certain all the continuous data is scaled properly with zero mean and unit standard deviaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# replace strings\n",
    "if df_test.income.dtype=='object':\n",
    "    df_test.income.replace(to_replace=[' <=50K.',' >50K.'],value=['<=50K','>50K'],inplace=True)\n",
    "    print(df_test.income.value_counts())\n",
    "\n",
    "# define objects that can encode each variable as integer    \n",
    "encoders = dict() \n",
    "categorical_headers = ['workclass','education','marital_status',\n",
    "                       'occupation','relationship','race','sex','country']\n",
    "\n",
    "# train all encoders (special case the target 'income')\n",
    "for col in categorical_headers+['income']:\n",
    "    df_train[col] = df_train[col].str.strip()\n",
    "    df_test[col] = df_test[col].str.strip()\n",
    "    \n",
    "    if col==\"income\":\n",
    "        tmp = LabelEncoder()\n",
    "        df_train[col] = tmp.fit_transform(df_train[col])\n",
    "        df_test[col] = tmp.transform(df_test[col])\n",
    "    else:\n",
    "        # integer encoded variables\n",
    "        encoders[col] = LabelEncoder() # save the encoder\n",
    "        df_train[col+'_int'] = encoders[col].fit_transform(df_train[col])\n",
    "        df_test[col+'_int'] = encoders[col].transform(df_test[col])\n",
    "\n",
    "# scale the numeric, continuous variables\n",
    "numeric_headers = [\"age\", \"cap_gain\", \"cap_loss\",\"work_hrs_weekly\"]\n",
    "\n",
    "for col in numeric_headers:\n",
    "    df_train[col] = df_train[col].astype(np.float)\n",
    "    df_test[col] = df_test[col].astype(np.float)\n",
    "    \n",
    "    ss = StandardScaler()\n",
    "    df_train[col] = ss.fit_transform(df_train[col].values.reshape(-1, 1))\n",
    "    df_test[col] = ss.transform(df_test[col].values.reshape(-1, 1))\n",
    "    \n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start as simply as possible, without any feature preprocessing\n",
    "categorical_headers_ints = [x+'_int' for x in categorical_headers]\n",
    "\n",
    "# we will forego one-hot encoding right now and instead just scale all inputs\n",
    "#   this is just to get an example running in Keras (don't ever do this)\n",
    "feature_columns = categorical_headers_ints+numeric_headers\n",
    "X_train =  ss.fit_transform(df_train[feature_columns].values).astype(np.float32)\n",
    "X_test =  ss.transform(df_test[feature_columns].values).astype(np.float32)\n",
    "\n",
    "y_train = df_train['income'].values.astype(np.int)\n",
    "y_test = df_test['income'].values.astype(np.int)\n",
    "\n",
    "print(feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![keras](https://blog.keras.io/img/keras-tensorflow-logo.jpg)\n",
    "\n",
    "\n",
    "# An example similar to Sklearn, Keras\n",
    "- We will start with creating a model that is similar to what we have already written. \n",
    "- Let's not worry about pre-processing the data right now. Let's just get familiar with Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as mt\n",
    "from tensorflow import keras\n",
    "\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Concatenate\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the features into a single large matrix\n",
    "X_train = df_train[feature_columns].to_numpy()\n",
    "X_test = df_test[feature_columns].to_numpy()\n",
    "\n",
    "# Now let's define the architecture for a multi-layer network\n",
    "\n",
    "# First, lets setup the input size\n",
    "num_features = X_train.shape[1]\n",
    "inputs = Input(shape=(num_features,))\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "x = Dense(units=10, activation='relu')(inputs)\n",
    "x = Dense(units=5, activation='tanh')(x)\n",
    "predictions = Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "# This creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "model = Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Adding more customization\n",
    "For this particular optimization, we could go beyond SGD and use a different optimizer. There are many excellent explanations of different optimizers, for instance:\n",
    "- http://sebastianruder.com/optimizing-gradient-descent/\n",
    "\n",
    "Now that we have defined the model, its as simple as using it in a very familiar syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# you will need to install pydot properly on your machine to get this running\n",
    "plot_model(\n",
    "    model, to_file='model.png', show_shapes=True, show_layer_names=True,\n",
    "    rankdir='LR', expand_nested=False, dpi=96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=50, verbose=1)\n",
    "\n",
    "# now lets see how well the model performed\n",
    "from sklearn import metrics as mt\n",
    "yhat_proba = model.predict(X_test)\n",
    "yhat = np.round(yhat_proba)\n",
    "print(mt.confusion_matrix(y_test,yhat))\n",
    "print(mt.classification_report(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch! It didn't even train! **But we are not handling categorical data properly here...** Let's fix that.\n",
    "\n",
    "## Embedding Categorical Data\n",
    "In order to add one-hot encoding, we need to separate the categorical features that are currently saved as integers and place them into Embedding layers. An embedding layer deals with integers as if they were one-hot encoded. To start, let's only create a model from these categorical variables. \n",
    "\n",
    "- The first decision to make is about the size of the dense feature embeddings. This is essentially a dimensionality reduction step. \n",
    " - This can be difficult to set, but one common setting is $\\log_2(N)$ or $\\sqrt{N}$ where $N$ is the total number of uniques values.\n",
    "- When using an Embedding, we can leave the variables represented as integers. That is, keras will use the integer representation to figure out how to one-hot-encode the variable. This is great because it means we don't need to change the variable in memory.\n",
    "- We will feed all the categorical variables to the model and, internally, the model will separate out the columns and apply a separate embedding. The function we will use to get each column is the `gather` function that is part of tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import concatenate\n",
    "import tensorflow as tf\n",
    "\n",
    "# start by getting only the categorical variables\n",
    "# these matrices are all integers\n",
    "X_train = df_train[categorical_headers_ints].to_numpy() \n",
    "X_test = df_test[categorical_headers_ints].to_numpy() \n",
    "\n",
    "embed_branches = []\n",
    "all_branch_outputs = [] # this is where we will keep track of output of each branch\n",
    "\n",
    "# feed in the entire matrix of categircal variables\n",
    "input_branch = Input(shape=(X_train.shape[1],), dtype='int64', name='categorical')\n",
    "\n",
    "# for each categorical variable\n",
    "for idx,col in enumerate(categorical_headers_ints):\n",
    "    \n",
    "    # what the maximum integer value for this variable?\n",
    "    # which is the same as the number of categories\n",
    "    N = max(df_train[col].max(),df_test[col].max())+1 \n",
    "    \n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_branch, idx, axis=1)\n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "    # save these outputs in list to concatenate later\n",
    "    all_branch_outputs.append(x)\n",
    "    \n",
    "# now concatenate the outputs and add a fully connected layer\n",
    "final_branch = concatenate(all_branch_outputs, name='concat_1')\n",
    "final_branch = Dense(units=1,activation='sigmoid', name='combined')(final_branch)\n",
    "\n",
    "model = Model(inputs=input_branch, outputs=final_branch)\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(\n",
    "    model, to_file='model.png', show_shapes=True, show_layer_names=True,\n",
    "    rankdir='LR', expand_nested=False, dpi=96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=50, verbose=1)\n",
    "\n",
    "from sklearn import metrics as mt\n",
    "yhat_proba = model.predict(X_test)\n",
    "yhat = np.round(yhat_proba)\n",
    "print(mt.confusion_matrix(y_test,yhat))\n",
    "print(mt.classification_report(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!! Looks like the use of Embedding layers helped. Lets add some more of the features, useing both the categorical and the integer features.\n",
    "\n",
    "## Multi-Modal Networks: Combine Branches, Sparse and Dense\n",
    "The keras functional API allows us to setup different branches of inputs. When we call the `fit` function, we can tell keras the input to each branch using a simple list (you can also use a dictionary to be more explicit).\n",
    "\n",
    "- Lets create two matrics: \n",
    " - one with the categorical data, `X_train_cat`\n",
    " - and another with the numeric data, `X_train_num`\n",
    " \n",
    "Now, we can feed both of these into the model as a list and concatenate their results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save categorical features\n",
    "X_train_cat = df_train[categorical_headers_ints].to_numpy() \n",
    "X_test_cat = df_test[categorical_headers_ints].to_numpy() \n",
    "\n",
    "# and save off the numeric features\n",
    "X_train_num =  df_train[numeric_headers].to_numpy()\n",
    "X_test_num = df_test[numeric_headers].to_numpy()\n",
    "\n",
    "all_branch_outputs = [] # this is where we will keep track of output of each branch\n",
    "\n",
    "input_cat = Input(shape=(X_train_cat.shape[1],), dtype='int64', name='categorical')\n",
    "for idx,col in enumerate(categorical_headers_ints):\n",
    "    \n",
    "    # track what the maximum integer value will be for this variable\n",
    "    # which is the same as the number of categories\n",
    "    N = max(df_train[col].max(),df_test[col].max())+1\n",
    "    \n",
    "    \n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_cat, idx, axis=1)\n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "    # save these outputs to concatenate later\n",
    "    all_branch_outputs.append(x)\n",
    "    \n",
    "# HERE IS THE ADDING OF AN INPUT USING NUMERIC DATA\n",
    "# create dense input branch for numeric\n",
    "inputs_num = Input(shape=(X_train_num.shape[1],), name='numeric')\n",
    "x_dense = Dense(units=15, activation='relu',name='num_1')(inputs_num)\n",
    "    \n",
    "all_branch_outputs.append(x_dense)\n",
    "\n",
    "# now concatenate the outputs and add a fully connected layer\n",
    "final_branch = concatenate(all_branch_outputs, name='concat_1')\n",
    "final_branch = Dense(units=1,activation='sigmoid', name='combined')(final_branch)\n",
    "\n",
    "model = Model(inputs=[input_cat,inputs_num], outputs=final_branch)\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(\n",
    "    model, to_file='model.png', show_shapes=True, show_layer_names=True,\n",
    "    rankdir='LR', expand_nested=False, dpi=96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit([ X_train_cat, X_train_num ], # inputs for each branch are a list\n",
    "          y_train, \n",
    "          epochs=10, \n",
    "          batch_size=50, \n",
    "          verbose=1)\n",
    "\n",
    "yhat = model.predict([X_test_cat,\n",
    "                      X_test_num]) # each branch has an input\n",
    "\n",
    "yhat = np.round(yhat)\n",
    "print(mt.confusion_matrix(y_test,yhat))\n",
    "print(mt.classification_report(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the confusion matrix is doing pretty well! But we still are just using a simple neural network. We really want to take advantage of the embeddings and crossed columns that are possible with tensorflow. \n",
    "\n",
    "\n",
    "**[Back to Slides]**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossed Columns and Deep Embeddings\n",
    "Lets create the network below in steps. This network consists of a number of different paths for the input data to take.\n",
    "![asdfasfd](https://www.tensorflow.org/images/wide_n_deep.svg)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "\n",
    "\n",
    "## Making Crossed Columns\n",
    "For this example, we are going to make embeddings for crossed columns. We will perform this step using `sklearn` \n",
    "\n",
    "Let's start simple with \n",
    "- branches of crossed categorical features as input \n",
    "- one branch for each crossed column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible crossing options:\n",
    "#   'workclass','education','marital_status',\n",
    "#   'occupation','relationship','race',\n",
    "#   'sex','country'\n",
    "\n",
    "cross_columns = [['education','occupation'],\n",
    "                 ['sex', 'marital_status'],\n",
    "                 ['workclass','occupation'],\n",
    "                 ['workclass','race','country']\n",
    "                ]\n",
    "\n",
    "# cross each set of columns in the list above\n",
    "cross_col_df_names = []\n",
    "for cols in cross_columns:\n",
    "    # encode as ints for the embedding\n",
    "    enc = LabelEncoder()\n",
    "    \n",
    "    # 1. create crossed labels by join operation\n",
    "    X_crossed_train = df_train[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "    X_crossed_test = df_test[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "    \n",
    "    # get a nice name for this new crossed column\n",
    "    cross_col_name = '_'.join(cols)\n",
    "    \n",
    "    # 2. encode as integers\n",
    "    enc.fit(np.hstack((X_crossed_train.to_numpy(),  X_crossed_test.to_numpy())))\n",
    "    \n",
    "    # 3. Save into dataframe with new name\n",
    "    df_train[cross_col_name] = enc.transform(X_crossed_train)\n",
    "    df_test[cross_col_name] = enc.transform(X_crossed_test)\n",
    "    \n",
    "    # keep track of the new names of the crossed columns\n",
    "    cross_col_df_names.append(cross_col_name) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get crossed columns\n",
    "X_train_crossed = df_train[cross_col_df_names].to_numpy()\n",
    "X_test_crossed = df_test[cross_col_df_names].to_numpy()\n",
    "\n",
    "crossed_outputs = [] # this is where we will keep track of output of each branch\n",
    "\n",
    "input_crossed = Input(shape=(X_train_crossed.shape[1],), dtype='int64', name='categorical')\n",
    "for idx,col in enumerate(cross_col_df_names):\n",
    "    \n",
    "    # track what the maximum integer value will be for this variable\n",
    "    # which is the same as the number of categories\n",
    "    N = max(df_train[col].max(),df_test[col].max())+1\n",
    "    \n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_crossed, idx, axis=1)\n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "    # save these outputs to concatenate later\n",
    "    crossed_outputs.append(x)\n",
    "    \n",
    "\n",
    "# now concatenate the outputs and add a fully connected layer\n",
    "wide_branch = concatenate(crossed_outputs, name='concat_1')\n",
    "wide_branch = Dense(units=1,activation='sigmoid', name='combined')(wide_branch)\n",
    "\n",
    "model = Model(inputs=input_crossed, outputs=wide_branch)\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_crossed,\n",
    "        y_train, epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = np.round(model.predict(X_test_crossed))\n",
    "print(mt.confusion_matrix(y_test,yhat))\n",
    "print(mt.classification_report(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will need to install pydot properly on your machine to get this running\n",
    "plot_model(\n",
    "    model, to_file='model.png', show_shapes=True, show_layer_names=True,\n",
    "    rankdir='LR', expand_nested=False, dpi=96\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! That is just using crossed columns and a one layer Linear Classifer! So memorization works fairly well here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "\n",
    "\n",
    "## Combining Crossed Linear Classifier and Deep Embeddings\n",
    "Now its just a matter of setting the wide and deep columns for tensorflow. After which, we can use the combined classifier!\n",
    "\n",
    "Wide and deep models can have really interesting and useful properties so they are great to keep in mind when selecting an architecture. Some of the hyperparameters that are specific to this are:\n",
    "- which features to cross together, typically you only want to cross columns you think are important to be connected--they somehow might create new knowledge by combining.\n",
    "- the size of the dense feature embeddings. This can be difficult to set, but one common setting is $log_2(N)$ or $\\sqrt{N}$ where $N$ is the total number of uniques values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get crossed columns\n",
    "X_train_crossed = df_train[cross_col_df_names].to_numpy()\n",
    "X_test_crossed = df_test[cross_col_df_names].to_numpy()\n",
    "\n",
    "# save categorical features\n",
    "X_train_cat = df_train[categorical_headers_ints].to_numpy() \n",
    "X_test_cat = df_test[categorical_headers_ints].to_numpy() \n",
    "\n",
    "# and save off the numeric features\n",
    "X_train_num =  df_train[numeric_headers].to_numpy()\n",
    "X_test_num = df_test[numeric_headers].to_numpy()\n",
    "\n",
    "\n",
    "# we need to create separate lists for each branch\n",
    "crossed_outputs = []\n",
    "\n",
    "# CROSSED DATA INPUT\n",
    "input_crossed = Input(shape=(X_train_crossed.shape[1],), dtype='int64', name='wide_inputs')\n",
    "for idx,col in enumerate(cross_col_df_names):\n",
    "    \n",
    "    # track what the maximum integer value will be for this variable\n",
    "    # which is the same as the number of categories\n",
    "    N = max(df_train[col].max(),df_test[col].max())+1\n",
    "    \n",
    "    \n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_crossed, idx, axis=1)\n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "    # save these outputs to concatenate later\n",
    "    crossed_outputs.append(x)\n",
    "    \n",
    "\n",
    "# now concatenate the outputs and add a fully connected layer\n",
    "wide_branch = concatenate(crossed_outputs, name='wide_concat')\n",
    "#wide_branch = Dense(units=1,activation='sigmoid', name='wide_combined')(wide_branch)\n",
    "\n",
    "# reset this input branch\n",
    "all_deep_branch_outputs = []\n",
    "\n",
    "# CATEGORICAL DATA INPUT\n",
    "input_cat = Input(shape=(X_train_cat.shape[1],), dtype='int64', name='categorical_input')\n",
    "for idx,col in enumerate(categorical_headers_ints):\n",
    "    \n",
    "    # track what the maximum integer value will be for this variable\n",
    "    # which is the same as the number of categories\n",
    "    N = max(df_train[col].max(),df_test[col].max())+1\n",
    "    \n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_cat, idx, axis=1)\n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "    # save these outputs to concatenate later\n",
    "    all_deep_branch_outputs.append(x)\n",
    "    \n",
    "# NUMERIC DATA INPUT\n",
    "# create dense input branch for numeric\n",
    "input_num = Input(shape=(X_train_num.shape[1],), name='numeric')\n",
    "x_dense = Dense(units=15, activation='relu',name='num_1')(input_num)\n",
    "    \n",
    "all_deep_branch_outputs.append(x_dense)\n",
    "\n",
    "\n",
    "# merge the deep branches together\n",
    "deep_branch = concatenate(all_deep_branch_outputs,name='concat_embeds')\n",
    "deep_branch = Dense(units=50,activation='relu', name='deep1')(deep_branch)\n",
    "deep_branch = Dense(units=25,activation='relu', name='deep2')(deep_branch)\n",
    "deep_branch = Dense(units=10,activation='relu', name='deep3')(deep_branch)\n",
    "    \n",
    "# merge the deep and wide branch\n",
    "final_branch = concatenate([wide_branch, deep_branch],name='concat_deep_wide')\n",
    "final_branch = Dense(units=1,activation='sigmoid',name='combined')(final_branch)\n",
    "\n",
    "model = Model(inputs=[input_crossed,input_cat,input_num], outputs=final_branch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will need to install pydot properly on your machine to get this running\n",
    "plot_model(\n",
    "    model, to_file='model.png', show_shapes=True, show_layer_names=True,\n",
    "    rankdir='LR', expand_nested=False, dpi=96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model.compile(optimizer='adagrad',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# lets also add the history variable to see how we are doing\n",
    "# and lets add a validation set to keep track of our progress\n",
    "history = model.fit([X_train_crossed,X_train_cat,X_train_num],\n",
    "                    y_train, \n",
    "                    epochs=15, \n",
    "                    batch_size=32, \n",
    "                    verbose=1, \n",
    "                    validation_data = ([X_test_crossed,X_test_cat,X_test_num],y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = np.round(model.predict([X_test_crossed,X_test_cat,X_test_num]))\n",
    "print(mt.confusion_matrix(y_test,yhat))\n",
    "print(mt.classification_report(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "\n",
    "plt.ylabel('Accuracy %')\n",
    "plt.title('Training')\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Validation')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.ylabel('Training Loss')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Generalization Better\n",
    "In what ways might we try to make the model found generalize more concretely?\n",
    "\n",
    "**Self Test:** What is the best method to make the results better?  \n",
    "- A. Add Dropout to deep network\n",
    "- B. Add Dropout to wide network\n",
    "- C. Perturb the Categorical Data during Training (augmentation)\n",
    "- D. Add L2 regularization to different layer's weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
